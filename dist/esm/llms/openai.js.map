{"version":3,"file":"openai.js","sources":["../../../src/llms/openai.ts"],"sourcesContent":["/**\n * OpenAI API client.\n *\n * This module contains functions used to make requests to the OpenAI API via\n * the Grafana LLM app plugin. That plugin must be installed, enabled and configured\n * in order for these functions to work.\n *\n * The {@link enabled} function can be used to check if the plugin is enabled and configured.\n */\n\nimport { isLiveChannelMessageEvent, LiveChannelAddress, LiveChannelMessageEvent, LiveChannelScope } from \"@grafana/data\";\nimport { getBackendSrv, getGrafanaLiveSrv, logDebug } from \"@grafana/runtime\";\n\nimport { pipe, Observable, UnaryFunction } from \"rxjs\";\nimport { filter, map, scan, takeWhile } from \"rxjs/operators\";\n\nconst LLM_PLUGIN_ID = 'grafana-llm-app';\nconst LLM_PLUGIN_ROUTE = `/api/plugins/${LLM_PLUGIN_ID}`;\nconst OPENAI_CHAT_COMPLETIONS_PATH = 'openai/v1/chat/completions';\n\n/** The role of a message's author. */\nexport type Role = 'system' | 'user' | 'assistant' | 'function';\n\n/** A message in a conversation. */\nexport interface Message {\n  /** The role of the message's author. */\n  role: Role;\n\n  /** The contents of the message. content is required for all messages, and may be null for assistant messages with function calls. */\n  content: string;\n\n  /**\n   * The name of the author of this message.\n   *\n   * This is required if role is 'function', and it should be the name of the function whose response is in the content.\n   *\n   * May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.\n   */\n  name?: string;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the model.\n   */\n  function_call?: Object;\n}\n\n/** A function the model may generate JSON inputs for. */\nexport interface Function {\n  /**\n   * The name of the function to be called.\n   *\n   * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n   */\n  name: string;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description?: string;\n  /*\n   * The parameters the functions accepts, described as a JSON Schema object. See the OpenAI guide for examples, and the JSON Schema reference for documentation about the format.\n   *\n   * To describe a function that accepts no parameters, provide the value {\"type\": \"object\", \"properties\": {}}.\n   */\n  parameters: Object;\n}\n\nexport interface ChatCompletionsRequest {\n  /**\n   * ID of the model to use.\n   *\n   * See the model endpoint compatibility table for details on which models work with the Chat Completions API.\n   */\n  model: string;\n  /** A list of messages comprising the conversation so far. */\n  messages: Message[];\n  /** A list of functions the model may generate JSON inputs for. */\n  functions?: Function[];\n  /**\n   * Controls how the model responds to function calls.\n   *\n   * \"none\" means the model does not call a function, and responds to the end-user.\n   * \"auto\" means the model can pick between an end-user or calling a function.\n   * Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function.\n   *\n   * \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n   */\n  function_call?: 'none' | 'auto' | { name: string };\n  /**\n   * What sampling temperature to use, between 0 and 2.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or top_p but not both.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n   * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number;\n  /**\n   * How many chat completion choices to generate for each input message.\n   */\n  n?: number;\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | string[];\n  /**\n   * The maximum number of tokens to generate in the chat completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.\n   */\n  max_tokens?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n   */\n  presence_penalty?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n   */\n  frequency_penalty?: number;\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.\n   * Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,\n   * but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: { [key: string]: number };\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.\n   */\n  user?: string;\n}\n\n/** A completion object from an OpenAI model. */\nexport interface Choice {\n  /** The message object generated by the model. */\n  message: Message;\n  /**\n   * The reason the model stopped generating text.\n   *\n   * This may be one of:\n   *  - stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n   *  - length: incomplete model output due to max_tokens parameter or token limit\n   *  - function_call: the model decided to call a function\n   *  - content_filter: omitted content due to a flag from our content filters\n   *  - null: API response still in progress or incomplete\n   */\n  finish_reason: string;\n  /** The index of the completion in the list of choices. */\n  index: number;\n}\n\n/** The usage statistics for a request to OpenAPI. */\nexport interface Usage {\n  /** The number of tokens in the prompt. */\n  prompt_tokens: number;\n  /** The number of tokens in the completion. */\n  completion_tokens: number;\n  /** The total number of tokens. */\n  total_tokens: number;\n}\n\n/** A response from the OpenAI Chat Completions API. */\nexport interface ChatCompletionsResponse<T = Choice> {\n  /** The ID of the request. */\n  id: string;\n  /** The type of object returned (e.g. 'chat.completion'). */\n  object: string;\n  /** The timestamp of the request, as a UNIX timestamp. */\n  created: number;\n  /** The name of the model used to generate the response. */\n  model: string;\n  /** A list of completion objects (only one, unless `n > 1` in the request). */\n  choices: T[];\n  /** The number of tokens used to generate the replies, counting prompt, completion, and total. */\n  usage: Usage;\n}\n\n/** A content message returned from the model. */\nexport interface ContentMessage {\n  /** The content of the message. */\n  content: string;\n}\n\n/** A message returned from the model indicating that it is done. */\nexport interface DoneMessage {\n  done: boolean;\n}\n\n/** A function call message returned from the model. */\nexport interface FunctionCallMessage {\n  /** The name of the function to call. */\n  name: string;\n  /** The arguments to the function call. */\n  arguments: any[];\n}\n\n/**\n * A delta returned from a stream of chat completion responses.\n *\n * In practice this will be either a content message or a function call;\n * done messages are filtered out by the `streamChatCompletions` function.\n */\nexport type ChatCompletionsDelta = ContentMessage | FunctionCallMessage | DoneMessage;\n\n/** A chunk included in a chat completion response. */\nexport interface ChatCompletionsChunk {\n  /** The delta since the previous chunk. */\n  delta: ChatCompletionsDelta;\n}\n\n/** Return true if the message is a 'content' message. */\nexport function isContentMessage(message: any): message is ContentMessage {\n  return message.content != null;\n}\n\n\n/** Return true if the message is a 'done' message. */\nexport function isDoneMessage(message: any): message is DoneMessage {\n  return message.done !== undefined\n}\n\n/**\n * An rxjs operator that extracts the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the content messages. Each emission will be a string containing the\n *         token emitted by the model.\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: 'gpt-3.5-turbo', messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe(console.log);\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n */\nexport function extractContent(): UnaryFunction<Observable<ChatCompletionsResponse<ChatCompletionsChunk>>, Observable<string>> {\n  return pipe(\n    filter((response: ChatCompletionsResponse<ChatCompletionsChunk>) => isContentMessage(response.choices[0].delta)),\n    // The type assertion is needed here because the type predicate above doesn't seem to propagate.\n    map((response: ChatCompletionsResponse<ChatCompletionsChunk>) => (response.choices[0].delta as ContentMessage).content),\n  )\n}\n\n/**\n * An rxjs operator that accumulates the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the accumulated content messages. Each emission will be a string containing the\n *         content of all messages received so far.\n * @example\n * const stream = streamChatCompletions({ model: 'gpt-3.5-turbo', messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe(console.log);\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function accumulateContent(): UnaryFunction<Observable<ChatCompletionsResponse<ChatCompletionsChunk>>, Observable<string>> {\n  return pipe(\n    extractContent(),\n    scan((acc, curr) => acc + curr, ''),\n  );\n}\n\n/**\n * Make a request to OpenAI's chat-completions API via the Grafana LLM plugin proxy.\n */\nexport async function chatCompletions(request: ChatCompletionsRequest): Promise<ChatCompletionsResponse> {\n  const response = await getBackendSrv().post<ChatCompletionsResponse>('/api/plugins/grafana-llm-app/resources/openai/v1/chat/completions', request, {\n    headers: { 'Content-Type': 'application/json' }\n  });\n  return response;\n}\n\n/**\n * Make a streaming request to OpenAI's chat-completions API via the Grafana LLM plugin proxy.\n *\n * A stream of tokens will be returned as an `Observable<string>`. Use the `extractContent` operator to\n * filter the stream to only content messages, or the `accumulateContent` operator to obtain a stream of\n * accumulated content messages.\n *\n * The 'done' message will not be emitted; the stream will simply end when this message is encountered.\n *\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: 'gpt-3.5-turbo', messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe(console.log);\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n *\n * @example <caption>Example of accumulating tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: 'gpt-3.5-turbo', messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe(console.log);\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function streamChatCompletions(request: ChatCompletionsRequest): Observable<ChatCompletionsResponse<ChatCompletionsChunk>> {\n  const channel: LiveChannelAddress = {\n    scope: LiveChannelScope.Plugin,\n    namespace: LLM_PLUGIN_ID,\n    path: OPENAI_CHAT_COMPLETIONS_PATH,\n    data: request,\n  };\n  const messages = getGrafanaLiveSrv()\n    .getStream(channel)\n    .pipe(filter((event) => isLiveChannelMessageEvent(event))) as Observable<LiveChannelMessageEvent<ChatCompletionsResponse<ChatCompletionsChunk>>>\n  return messages.pipe(\n    takeWhile((event) => !isDoneMessage(event.message.choices[0].delta)),\n    map((event) => event.message),\n  );\n}\n\nlet loggedWarning = false;\n\n/** Check if the OpenAI API is enabled via the LLM plugin. */\nexport const enabled = async () => {\n  try {\n    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, undefined, undefined, {\n      showSuccessAlert: false, showErrorAlert: false,\n    });\n    return settings.enabled && (settings?.secureJsonFields?.openAIKey ?? false);\n  } catch (e) {\n    if (!loggedWarning) {\n      logDebug(String(e));\n      logDebug('Failed to check if OpenAI is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.');\n      loggedWarning = true;\n    }\n    return false;\n  }\n}\n"],"names":[],"mappings":";;;;;AAgBA,MAAM,aAAgB,GAAA,iBAAA,CAAA;AACtB,MAAM,mBAAmB,CAAgB,aAAA,EAAA,aAAA,CAAA,CAAA,CAAA;AACzC,MAAM,4BAA+B,GAAA,4BAAA,CAAA;AA2M9B,SAAS,iBAAiB,OAAyC,EAAA;AACxE,EAAA,OAAO,QAAQ,OAAW,IAAA,IAAA,CAAA;AAC5B,CAAA;AAIO,SAAS,cAAc,OAAsC,EAAA;AAClE,EAAA,OAAO,QAAQ,IAAS,KAAA,KAAA,CAAA,CAAA;AAC1B,CAAA;AAgBO,SAAS,cAA+G,GAAA;AAC7H,EAAO,OAAA,IAAA;AAAA,IACL,MAAA,CAAO,CAAC,QAA4D,KAAA,gBAAA,CAAiB,SAAS,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA;AAAA,IAE/G,GAAA,CAAI,CAAC,QAA6D,KAAA,QAAA,CAAS,QAAQ,CAAC,CAAA,CAAE,MAAyB,OAAO,CAAA;AAAA,GACxH,CAAA;AACF,CAAA;AAgBO,SAAS,iBAAkH,GAAA;AAChI,EAAO,OAAA,IAAA;AAAA,IACL,cAAe,EAAA;AAAA,IACf,KAAK,CAAC,GAAA,EAAK,IAAS,KAAA,GAAA,GAAM,MAAM,EAAE,CAAA;AAAA,GACpC,CAAA;AACF,CAAA;AAKA,eAAsB,gBAAgB,OAAmE,EAAA;AACvG,EAAA,MAAM,WAAW,MAAM,aAAA,EAAgB,CAAA,IAAA,CAA8B,qEAAqE,OAAS,EAAA;AAAA,IACjJ,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAmB,EAAA;AAAA,GAC/C,CAAA,CAAA;AACD,EAAO,OAAA,QAAA,CAAA;AACT,CAAA;AA6BO,SAAS,sBAAsB,OAA4F,EAAA;AAChI,EAAA,MAAM,OAA8B,GAAA;AAAA,IAClC,OAAO,gBAAiB,CAAA,MAAA;AAAA,IACxB,SAAW,EAAA,aAAA;AAAA,IACX,IAAM,EAAA,4BAAA;AAAA,IACN,IAAM,EAAA,OAAA;AAAA,GACR,CAAA;AACA,EAAA,MAAM,QAAW,GAAA,iBAAA,EACd,CAAA,SAAA,CAAU,OAAO,CAAA,CACjB,IAAK,CAAA,MAAA,CAAO,CAAC,KAAA,KAAU,yBAA0B,CAAA,KAAK,CAAC,CAAC,CAAA,CAAA;AAC3D,EAAA,OAAO,QAAS,CAAA,IAAA;AAAA,IACd,SAAA,CAAU,CAAC,KAAA,KAAU,CAAC,aAAA,CAAc,KAAM,CAAA,OAAA,CAAQ,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA,IACnE,GAAI,CAAA,CAAC,KAAU,KAAA,KAAA,CAAM,OAAO,CAAA;AAAA,GAC9B,CAAA;AACF,CAAA;AAEA,IAAI,aAAgB,GAAA,KAAA,CAAA;AAGb,MAAM,UAAU,YAAY;AA1UnC,EAAA,IAAA,EAAA,EAAA,EAAA,CAAA;AA2UE,EAAI,IAAA;AACF,IAAM,MAAA,QAAA,GAAW,MAAM,aAAc,EAAA,CAAE,IAAI,CAAG,EAAA,gBAAA,CAAA,SAAA,CAAA,EAA6B,QAAW,KAAW,CAAA,EAAA;AAAA,MAC/F,gBAAkB,EAAA,KAAA;AAAA,MAAO,cAAgB,EAAA,KAAA;AAAA,KAC1C,CAAA,CAAA;AACD,IAAA,OAAO,SAAS,OAAY,KAAA,CAAA,EAAA,GAAA,CAAA,EAAA,GAAA,QAAA,IAAA,IAAA,GAAA,KAAA,CAAA,GAAA,QAAA,CAAU,gBAAV,KAAA,IAAA,GAAA,KAAA,CAAA,GAAA,EAAA,CAA4B,cAA5B,IAAyC,GAAA,EAAA,GAAA,KAAA,CAAA,CAAA;AAAA,WAC9D,CAAP,EAAA;AACA,IAAA,IAAI,CAAC,aAAe,EAAA;AAClB,MAAS,QAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA,CAAA;AAClB,MAAA,QAAA,CAAS,wIAAwI,CAAA,CAAA;AACjJ,MAAgB,aAAA,GAAA,IAAA,CAAA;AAAA,KAClB;AACA,IAAO,OAAA,KAAA,CAAA;AAAA,GACT;AACF;;;;"}